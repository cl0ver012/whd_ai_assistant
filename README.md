# Marketing Data to Supabase - Zero Data Loss

Process your **Google Ads** and **TikTok Ads** CSV exports and store them in Supabase with **100% data preservation**.

## ğŸ“Š Supported Platforms

- âœ… **Google Ads** - Performance & conversion data
- âœ… **TikTok Ads** - Campaign performance & video metrics
- âœ… **Meta Ads** - Campaign performance & video metrics
- âœ… **Power BI** - Store sales data (LFL Sales)

## ğŸ¯ Two Versions Available

### âš¡ **Simple Version** (RECOMMENDED TO START)
- âœ… **Fast & Easy** - No embeddings, no Google API needed
- âœ… **Zero Data Loss** - 100% preservation
- âœ… **Only 2 credentials** - Just Supabase URL & Key
- âœ… **10x faster** - Process 3,000 rows in ~5-10 minutes
- ğŸ‘‰ **[START HERE: SIMPLE_SETUP.md](SIMPLE_SETUP.md)**

### ğŸ¤– **Full Version** (With AI Embeddings)
- âœ… **Zero Data Loss** + Vector Embeddings
- âœ… **AI-Ready** - Semantic search capabilities
- âœ… **3 credentials** - Supabase + Google Gemini
- â±ï¸ **Slower** - ~25-40 minutes for 3,000 rows
- ğŸ“– See instructions below

## ğŸ“¦ What Gets Stored

For each CSV row, you get **4 layers of data**:

```
1. Raw Original Data   â†’ Exact CSV values (with commas, %, etc.)
2. Processed Metadata  â†’ Cleaned for SQL queries
3. Text Content        â†’ Human-readable summary
4. Vector Embedding    â†’ 768-dim for semantic search
```

**Example:**

```json
{
  "raw_data": {
    "original_row": {
      "Day": "2024-11-01",
      "Campaign": "[WHD] Display - Atlanta USA",
      "Impr.": "1,618",     // â† Original with comma
      "CTR": "1.85%",       // â† Original with %
      ...all 13 columns
    }
  },
  "metadata": {
    "day": "2024-11-01",
    "campaign": "[WHD] Display - Atlanta USA",
    "impressions": 1618,    // â† Cleaned for queries
    "ctr": 1.85,            // â† Cleaned for queries
    ...all fields typed
  },
  "embedding": [0.123, -0.456, ...]  // â† 768 dimensions
}
```

---

## âš¡ SIMPLE VERSION (Recommended)

**No embeddings, just data storage - fast and easy!**

### Quick Start

```bash
# 1. Install
pip install -r requirements_simple.txt

# 2. Setup Supabase (run setup_supabase_simple.sql)

# 3. Create .env with just 2 credentials:
SUPABASE_URL=your_url
SUPABASE_KEY=your_key

# 4. Test
python test_setup_simple.py

# 5. Process
python process_google_ads_simple.py
```

**ğŸ“– Full instructions:** [SIMPLE_SETUP.md](SIMPLE_SETUP.md)

---

## ğŸ¤– FULL VERSION (With Embeddings)

**Includes vector embeddings for AI/RAG applications**

### Quick Start

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Set Up Supabase

1. Go to your Supabase project
2. Click **"SQL Editor"** in left sidebar
3. Click **"+ New query"** and paste contents of `setup_supabase_enhanced.sql`
4. Click **"Run"**

**ğŸ“– Need help running SQL? See [HOW_TO_RUN_SQL.md](HOW_TO_RUN_SQL.md)**

This creates the `google_ads_documents` table with vector embedding support.

### Step 3: Configure Environment

Create a `.env` file:

```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
GOOGLE_API_KEY=AIzaSyD...
```

**ğŸ“– Need help getting credentials?**
- **Supabase**: Settings â†’ API â†’ Project URL & anon/public key
- **Google Gemini**: [AI Studio](https://makersuite.google.com/app/apikey)
- **Detailed guide**: [CREDENTIALS_GUIDE.md](CREDENTIALS_GUIDE.md)

### Step 4: Test Setup

```bash
python test_setup.py
```

Should see all âœ… green checkmarks.

### Step 5: Process Your Data

```bash
python process_google_ads_zero_loss.py
```

This will:
- Read all CSV files from `NBX/Google Ads Export/`
- Store raw original data (100% preservation)
- Create processed metadata (for queries)
- Generate embeddings (for AI search)
- Store everything in Supabase

**Time:** ~25-40 minutes for ~3,000 rows (includes API rate limiting)

## ğŸ“Š Data Sources Supported

### Performance Files
`google_ads_performance_*.csv` - 13 properties per row:
- Day, Campaign, Campaign type, Ad group, Landing page
- Currency code, Cost, Impressions, Clicks
- CTR, Avg. CPC, Conversions, Conversion rate

### Action Files
`google_ads_actions_*.csv` - 5 properties per row:
- Day, Campaign, Ad group, Conversion action, Conversions

## ğŸ” Query Examples

### Access Raw Original Data

```sql
-- Get original values exactly as in CSV
SELECT 
    raw_data->'original_row'->>'Impr.' as impressions,
    raw_data->'original_row'->>'CTR' as ctr
FROM google_ads_documents
LIMIT 5;

-- Result: "1,618" and "1.85%" (original format)
```

### Query Processed Data

```sql
-- Calculate totals
SELECT 
    metadata->>'campaign' as campaign,
    SUM((metadata->>'cost')::float) as total_cost,
    SUM((metadata->>'clicks')::integer) as total_clicks
FROM google_ads_documents
WHERE metadata->>'period' = '2024_11'
GROUP BY metadata->>'campaign'
ORDER BY total_cost DESC;
```

### Filter and Aggregate

```sql
-- Find high-performing days
SELECT 
    metadata->>'day' as date,
    metadata->>'campaign' as campaign,
    (metadata->>'ctr')::float as ctr,
    (metadata->>'conversions')::float as conversions
FROM google_ads_documents
WHERE (metadata->>'ctr')::float > 2.0
  AND (metadata->>'conversions')::float > 0
ORDER BY (metadata->>'ctr')::float DESC;
```

### Semantic Search

```python
# In Python
from supabase import create_client
import google.generativeai as genai

# Generate query embedding
query = "high performing display campaigns with conversions"
embedding = genai.embed_content(
    model="models/embedding-001",
    content=query,
    task_type="retrieval_query"
)

# Search
result = supabase.rpc(
    'match_google_ads_documents',
    {
        'query_embedding': embedding['embedding'],
        'match_count': 5
    }
).execute()
```

## âœ… Verify Zero Data Loss

After processing, verify nothing was lost:

```sql
-- Check raw data exists
SELECT COUNT(*) FROM google_ads_documents 
WHERE raw_data IS NOT NULL;

-- View original CSV format
SELECT 
    raw_data->'original_row' as original_csv_row
FROM google_ads_documents
LIMIT 1;

-- Verify all columns present
SELECT 
    jsonb_object_keys(raw_data->'original_row') as csv_columns
FROM google_ads_documents
LIMIT 1;
```

## ğŸ“ File Structure

```
Essential Files:
â”œâ”€â”€ README.md                          # This file (overview)
â”œâ”€â”€ SIMPLE_SETUP.md                    # âš¡ Simple version guide
â”œâ”€â”€ process_google_ads_simple.py       # âš¡ Simple processor (no embeddings)
â”œâ”€â”€ setup_supabase_simple.sql          # âš¡ Simple database setup
â”œâ”€â”€ test_setup_simple.py               # âš¡ Simple test
â”œâ”€â”€ requirements_simple.txt            # âš¡ Simple dependencies
â”‚
â”œâ”€â”€ process_google_ads_zero_loss.py    # ğŸ¤– Full processor (with embeddings)
â”œâ”€â”€ setup_supabase_enhanced.sql        # ğŸ¤– Full database setup
â”œâ”€â”€ test_setup.py                      # ğŸ¤– Full test
â”œâ”€â”€ requirements.txt                   # ğŸ¤– Full dependencies
â”‚
â”œâ”€â”€ CREDENTIALS_GUIDE.md               # How to get API keys
â”œâ”€â”€ ZERO_DATA_LOSS_GUIDE.md           # Detailed guide
â””â”€â”€ PROJECT_STRUCTURE.md               # Quick reference
```

## ğŸ“ For RAG AI Assistants

This approach is perfect for RAG because:

1. **Rich Context** - AI gets full campaign details
2. **Semantic Search** - Find relevant data by meaning
3. **Flexible Queries** - Answer diverse user questions
4. **Exact Values** - Can cite original numbers
5. **No Hallucination** - Ground truth in raw_data

### Example AI Queries Your System Can Answer:

- "What was our total spend on November 15th?"
- "Which ad groups have the best CTR?"
- "Show me conversion trends for Q4 2024"
- "Compare Display vs YouTube campaign performance"
- "What's our cost per conversion for Atlanta campaigns?"
- "Find similar high-performing campaigns"

## ğŸ’¾ Storage Requirements

**Per row:** ~4.7 KB  
**For 3,000 rows:** ~14 MB total  
**Extra cost for zero loss:** ~1.4 MB (only 10% overhead!)

## ğŸ”§ Customization

Want to add custom fields? Edit `process_google_ads_zero_loss.py`:

```python
# Add to metadata
metadata = {
    ...existing fields...,
    'custom_field': compute_something(row),
    'custom_tag': 'your_value'
}
```

## ğŸ†˜ Troubleshooting

### "Table does not exist"
â†’ Run `setup_supabase_enhanced.sql` in Supabase SQL Editor

### "API key invalid"
â†’ Check your `.env` file has correct keys (no quotes, no spaces)

### "Module not found"
â†’ Run `pip install -r requirements.txt`

### "Rate limit exceeded"
â†’ Normal! Script has built-in delays. Increase `time.sleep()` values if needed

### "Can't find data folder"
â†’ Ensure `NBX/Google Ads Export/` folder exists with CSV files

## ğŸ“– Additional Documentation

**Google Ads:**
- **`CREDENTIALS_GUIDE.md`** - Step-by-step guide to get all API credentials
- **`ZERO_DATA_LOSS_GUIDE.md`** - Complete guide with examples and verification steps
- **`SIMPLE_SETUP.md`** - Quick start for simple version

**TikTok Ads:**
- **`TIKTOK_SETUP_GUIDE.md`** - Complete setup guide for TikTok Ads processing
- **`setup_supabase_tiktok.sql`** - Database schema for TikTok data

**Power BI:**
- **`POWERBI_SETUP_GUIDE.md`** - Complete setup guide for Power BI processing
- **`setup_powerbi.sql`** - Database schema for Power BI data
- **`process_powerbi.py`** - Processing script (no embeddings)

**General:**
- **`PROJECT_STRUCTURE.md`** - Quick reference for project layout
- **`HOW_TO_RUN_SQL.md`** - Guide for running SQL scripts

## ğŸš€ Next Steps

After processing your marketing data:

1. âœ… **Process TikTok Ads** - See [TIKTOK_SETUP_GUIDE.md](TIKTOK_SETUP_GUIDE.md)
2. âœ… Build semantic search interface
3. âœ… Create RAG AI Assistant
4. âœ… Add automated data updates
5. âœ… Build analytics dashboards
6. âœ… Compare cross-platform performance

## ğŸ” Security Notes

- Never commit your `.env` file
- Use environment variables for all credentials
- Consider Row Level Security (RLS) in Supabase for production

## ğŸ“Š Database Schema

```sql
google_ads_documents (
  id          BIGSERIAL PRIMARY KEY,
  content     TEXT NOT NULL,
  metadata    JSONB NOT NULL,
  raw_data    JSONB NOT NULL,      -- â† Original CSV data
  embedding   vector(768),
  created_at  TIMESTAMP,
  updated_at  TIMESTAMP
)
```

## ğŸ¯ Key Features

- âœ… Zero data loss - original CSV preserved
- âœ… Efficient queries - typed metadata
- âœ… Semantic search - vector embeddings
- âœ… Audit trail - can reconstruct original files
- âœ… Future-proof - add new queries anytime
- âœ… Cost-effective - minimal storage overhead

## ğŸ¤ Support

- Check `ZERO_DATA_LOSS_GUIDE.md` for detailed documentation
- Review verification queries to ensure data integrity
- Test queries on small sample before production use

## ğŸ“ License

MIT

---

**Ready to build your RAG AI Assistant with zero data loss!** ğŸš€

Run `python process_google_ads_zero_loss.py` to get started.

